gpt2-chatbot
This page is a work in progress. Its conclusions are likely to change as more information is collected.
2023-04-30: Restructured and revised the content significantly. Added: references, testing prompts, example output, images.
NEWS: gpt2-chatbot is extremely likely to run on a server operated by, or associated with, OpenAI. This was determined by comparing API error messages.

Background
https://chat.lmsys.org enables users to chat with various LLMs and rate their output, without needing to log in. One of the models recently available is gpt2-chatbot, which demonstrates capability greatly beyond that of any known GPT-2 model. It is available for chatting with in the "Direct Chat", and also in "Arena (Battle)" which is the (initially) blinded version for bench-marking. There is no information to be found on that particular model name anywhere on the site, or elsewhere. The results generated by LMSYS benchmarks are available via their API for all models - except for this one.

Quick Rundown
gpt2-chatbot is a model that is capable of providing remarkably informative, rational, and relevant replies. The average output quality across many different domains places it on, at least, the same level as high-end models such as GPT-4 and Claude Opus.
It uses OpenAI's tiktoken tokenizer; this has been verified by comparing the effect of those special tokens on gpt2-chatbot and multiple other models, in the "Special Token Usage" section below).
Its assistant instruction has been extracted, and specifies that it's based off the GPT-4 architecture and has "Personality: v2".
When "provider" contact details are demanded, it provides highly detailed contact information to OpenAI (in greater detail than GPT-3.5/4).
It claims to be "based on GPT-4", and refers to itself as "ChatGPT" - or "a ChatGPT". The way it presents itself is generally distinct from the hallucinated replies from other organizations' models that have been trained on datasets created by OpenAI models.
It exhibits OpenAI-specific prompt injection vulnerabilities, and has not once claimed to belong to any other entity than OpenAI.
It is possible that the autobiographical information is merely a hallucination, or stem from instructions incorrectly provided to it.
Models from Anthropic, Meta, Mistral, Google, et c regularly provide different output than gpt2-chatbot for the same prompts.
Subjective notes
It appears quite likely that this mystery model is an early version of GPT-4.5 (not GPT-5), as part of another line of "incremental" model updates from OpenAI. The quality of the output in general - in particular its formatting, verbosity, structure, and overall comprehension - is absolutely superb. Multiple individuals, with great LLM prompting and chat-bot experience, have noted unexpectedly good quality of the output (in public and in private) - and I fully agree. To me the model feels like the step from GPT-3.5 to GPT-4, but instead using GPT-4 as a starting point. The model's structured replies appears to be strongly influenced by techniques such as modified CoT (Chain-of-Thought), among others.

There is currently no good reason to believe that that the mystery model uses some entirely new architecture. The possibility that LMSYS have set up something conceptually similar to a MoE (Mixture of Experts), acting as a router (adapter) for their connected models, has not been investigated. It is possible that LMSYS has trained a model of their own, as discussed below. The simplest explanation may be that this is the result of some kind of incorrect service configuration within LMSYS. I encourage people to remain skeptic, be aware of confirmation bias, and maintain an evidence-based mindset.

As a result of publishing this rentry, there has been quite a bit of discussion online regarding the possible OpenAI/gpt2-chatbot connection. Earlier today, Sam Altman posted a tweet that was quickly edited, which cannot reasonably be anything but a reference to the discussion. While some have considered this to be a "soft endorsement" of their connection to the model, I do not believe it indicates anything in particular. A vague and non-committal comment of that nature contributes to the hype, and serves their goals, regardless of whether the discussion is well-founded or not.

Possible motivations
This particular model could be a "stealth drop" by OpenAI to benchmark their latest GPT model, without making it apparent that it's on lmsys.org. The purpose of this could then be to: a) get replies that are "ordinary benchmark" tests without people intentionally seeking out GPT-4.5/5, b) avoid ratings that may be biased due to elevated expectations, which could cause people to rate it more negatively, and to a lesser extent c) decrease the likelihood of getting "mass-downvoted"/dogpiled by other competing entities. OpenAI would provide provide the compute while LMSYS provides the front-end as usual, while they are provided with unusually high-quality datasets from user interaction.

Other options
Something I would put in the realm of "pretty much impossible" rather than "plausible" is the notion that gpt2-chatbot could be based off the GPT-2 architecture. The main reason for even bringing this up is that a recent (April 7, 2024) article, from Meta/FAIR Labs and Mohamed bin Zayed University of AI (MBZUAI), titled Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws studied particulars of the GPT-2 architecture in-depth and established that:

"The GPT-2 architecture, with rotary embedding, matches or even surpasses LLaMA/Mistral architectures in knowledge storage, particularly over shorter training durations. This arises because LLaMA/Mistral uses GatedMLP, which is less stable and harder to train."

If LMSYS were the model creators, an application of some of the results of that article could then utilize datasets generated via LMSYS for training, among others. The model's strong tendency to "identify" as GPT-4 could then be explained by utilizing mainly datasets generated by GPT-4. The above connection is notable given that MBZUAI is a sponsor of LMSYS, as can be seen on their webpage:

Analysis of Service-specific Error Messages
Sending prompts that are malformed somehow can make a chat API service refuse the request, to prevent it from malfunctioning, and the returned error messages will often provide useful information. Chat models that run on OpenAI servers return certain service-specific error codes. The following is an openai.BadRequestError from openai-python.

(error_code: 50004, Error code: 400 - {'error': {'message': "Sorry! We've encountered an issue with repetitive patterns in your prompt. Please try again with a different prompt.", 'type': 'invalid_prompt', 'param': 'prompt', 'code': None}})

That specific error code is has exclusively been demonstrated to be returned from confirmed OpenAI models available on LMSYS - and for gpt2-chatbot. If this result holds up to further testing, it will be an extremely strong indication that gpt2-chatbot runs on OpenAI-managed (or at least connected) server, and that it is almost guaranteed to be an OpenAI model.

Chat models like LLaMA or Yi, for example, will instead provide the following error message for the same prompt as above, which is an error message from rayllm - which stems from AnyScale that provides an endpoint for LLaMA inference:

rayllm.backend.llm.error_handling.PromptTooLongError [...]
(error_code: 50004, Error code: 400 - {'error': {'message': 'Input validation error: inputs tokens + max_new_tokens must be <= 4097 [...]

You could verify or disprove these claims by reading API documentation - or hypothetically by recreating such error messages on your own by sending extremely long messages to different models, but that is of course not something I recommend.

Rate Limits
"GPT2-chatbot" has a rate limit that is different from the GPT-4 models, for direct chat:
1
2
3
MODEL_HOURLY_LIMIT (gpt-4-turbo-2024-04-09): 200  [=4800 replies per day, service total]
MODEL_HOURLY_LIMIT (gpt-4-1106-preview): 100      [= 2400 replies per day, service total]
USER_DAILY_LIMIT (gpt2-chatbot): 8                [per user]
The full restrictions on total vs user-specific rate limits have not yet been compared. If this daily user limit, or some other total service limit, is in fact more restrictive than for the GPT-4 models - this could imply that the model is more costly in terms of compute, and that the ones providing the compute prefer users to use the Arena (Battle) mode for generating benchmarks. Battle mode is what people get directed to go once they hit the daily user limit.

How you can help research this
Use the chat link and choose gpt2-chatbot. You can test the direct chat for 8 messages, and then switch to "Battle" mode and either select or re-roll model options. Feel free to click the feedback buttons; there's probably some minimal amount of ratings you need to provide, to be able to keep using that chat mode. Note: the following error message is created by LMSYS' own moderation, just like the "<span>" code displayed during text generation, and is not model-specific:
"$MODERATION$ YOUR INPUT VIOLATES OUR CONTENT MODERATION GUIDELINES."

Substantial feedback and critique is welcomed. You're encouraged to attempt to prove anything here as incorrect.
For all prompts you try, attempt to re-roll at least three times to get a mean impression of the model's capability. You should generally start with an empty context window, by clicking "New Round". Examples of simple prompts for testing model capability include Draw a unicorn in TiKZ. [3] and On the surface of the Moon, what has the greatest mass: 2 kg of lead or 1 kg of feathers?. Note that restricting how a model should reply ("Only write out the answer") is a significant source of error, as this prevents the model from "thinking explicitly" by writing their reasoning out.

Instruction